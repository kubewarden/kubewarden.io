<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><title>Kubewarden 1.13 release</title><link rel=icon type=image/svg href=/favicon.svg><link rel=mask-icon href=/favicon.svg><link rel=stylesheet type=text/css href=/sass/blog.min.css?1743405574><script async defer src=https://buttons.github.io/buttons.js></script><script type=text/javascript src=//cdn.bizible.com/scripts/bizible.js async></script></head><body><header><div class=sub-header><a href><img src=/images/logo-kubewarden.svg alt=Kubewarden></a>
<a href=#main-menu id=main-menu-toggle class=menu-toggle aria-label="Open main menu"><span aria-hidden=true></span><span aria-hidden=true></span><span aria-hidden=true></span></a><nav id=main-menu class=main-menu aria-label="Main menu"><a href=#main-menu-toggle id=main-menu-close class=menu-close aria-label="Close main menu"><span class=sr-only>Close main menu</span>
<span class="fa fa-close" aria-hidden=true></span></a><h2 id=main-menu-heading class=sr-only>Main menu</h2><ul class=menu><li><a href="https://artifacthub.io/packages/search?kind=13&sort=relevance&page=1" target=_blank class="btn no-bg">Policies</a></li><li><a href=https://charts.kubewarden.io/ target=_blank class="btn no-bg">Helm Charts</a></li><li><a href=https://docs.kubewarden.io/ target=_blank class="btn no-bg">Docs</a></li><li><a href="https://kubernetes.slack.com/?redir=%2Fmessages%2Fkubewarden" target=_blank class="btn no-bg">Slack</a></li><li><a href=https://bsky.app/profile/kubewarden.io target=_blank class="btn no-bg">Bluesky</a></li><li><a href=https://hachyderm.io/@kubewarden rel=me target=_blank class="btn no-bg">Mastodon</a></li><li><a href=/#get-in-touch target=_blank class="btn no-bg">Community</a></li><li><a href=/blog target=_blank class="btn no-bg">Blog</a></li><li><a href=https://docs.kubewarden.io/enterprise target=_blank class="btn no-bg">Enterprise</a></li><li><a href=https://github.com/kubewarden class="btn github"><img src=/images/icon-github.svg><span>Github</span></a></li></ul></nav><a href=#main-menu-toggle class=backdrop tabindex=-1 aria-hidden=true hidden></a></div></header><link rel=stylesheet href=/css/blog.css><section class=intro><div class=bg-primary><div class="wrap grid-one"><div><h1 style=color:#fff><a href=/blog>The Kubewarden blog</a>
<a href=/blog/index.xml><img src=/images/icon-rss.svg class=fill-white height=20></a></h1></div></div></div></section><main><section class=wrap><article><h1>Kubewarden 1.13 release</h1><p>Author: <author>Flavio Castelli</author></p><p>Published: <time>06 Jun 2024</time></p><p>Updated: <time>31 Mar 2025</time></p><div id=blog-post-content><p>I&rsquo;m pleased to announce a new release of Kubewarden, version 1.13. This release features a series of
improvements and bug fixes that contribute to better performance and stability.</p><p>Let&rsquo;s go through the most significant changes.</p><h2 id=policy-server-memory-usage>Policy Server memory usage</h2><p>A community member reported that the Kubewarden Policy Server was using a lot of memory, especially
when running context aware policies on big clusters. The number of resources being accessed by the
policies was significantly high, in the order of 3200 Namespaces, 10500 Ingresses, 200 ClusterRoleBindings
and 11000 RoleBindings.
Under certain circumstances, the amount of memory being used caused the Policy Server to be terminated by the kernel&rsquo;s OOM killer.</p><p>After some investigation, we found two different root causes of this issue.</p><p>The first one was related to <a href=https://github.com/kube-rs>kube-rs</a>, the Rust library we use to interact with Kubernetes.
The library was doing excessive allocations when building the initial state of a resource. We fixed this issue
with <a href=https://github.com/kube-rs/kube/pull/1494>this PR</a>.
With this fix we reduced the peak of memory usage caused by the creation of the initial state of a Kubernetes resource.</p><p>The second issue was related with how the memory allocator of Rust works. By default, Rust uses the system allocator (glibc malloc in our case).
It turns out the system allocator is not releasing the allocated memory back to the system when this is no longer needed.
We changed Policy Server to use the <a href=http://jemalloc.net/>jemalloc</a> allocator which, on top of providing great performance, has many
configuration knobs.
After some experimentation, we found the right balance between performance and memory usage.</p><p>With these two changes, we were able to reduce the peak of memory consumption of the Policy Server significantly, plus we reduced the
idle memory consumption.
As a pleasant side effect, we also noticed an improvement in the performances of the Policy Server thanks to the usage of jemalloc.</p><p>These are some numbers we obtained while doing performance tests of a Gatekeeper policy fetching, among other resources, 10000 RoleBindings:</p><table><thead><tr><th></th><th style=text-align:center>HTTP Request Duration (avg)</th><th style=text-align:center>Max RSS Under load</th><th style=text-align:center>Idle RSS</th></tr></thead><tbody><tr><td>Policy Server 1.12</td><td style=text-align:center>436.15ms</td><td style=text-align:center>1.4 Gb</td><td style=text-align:center>1.2 Gb</td></tr><tr><td>Policy Server 1.13</td><td style=text-align:center>233.663ms</td><td style=text-align:center>1.2 Gb</td><td style=text-align:center>264 Mb</td></tr></tbody></table><h2 id=audit-scanner-improvements>Audit Scanner improvements</h2><p>The Audit Scanner runs regularly inside Kubernetes clusters to determine the compliance status of its resources.
It does that by issuing many HTTP requests against the Policy Servers. A Policy Server can be backed by multiple replicas of
the Policy Server Pod.</p><p>We got reports from users that the Audit Scanner was always talking with only one of the replicas. This caused the following issues.
First, the audit scan was slow. Second, when running with multiple replicas, one of the Policy Server Pods was overloaded with requests,
while the other ones were idle.</p><p>This issue was caused by how the Go HTTP client works. By default, the Go HTTP client uses a connection pool to reuse HTTP connections.
Once we turned off this behavior we immediately saw the requests being spread between the different replicas of the Policy Server.</p><p>Note: the Audit Scanner accesses the Policy Server by going through the <code>ClusterIP</code> of the Service that exposes that Policy Server internally.
The <code>kube-proxy</code> component is responsible for load balancing the requests between the different Pods of the Policy Server.</p><p>Once this limitation was removed, we saw a significant reduction of the processing time of the Audit Scanner, especially on clusters with many policies and
resources to evaluate.</p><p>However, not all the operations done by the Audit Scanner were happening in parallel. Starting from this release, all the operations done by the Audit Scanner
are done in parallel. The amount of parallelism can be configured by the end user with different flags.
Please refer to the <a href=https://github.com/kubewarden/audit-scanner/blob/v1.13.0/README.md#tuning>official documentation</a> to learn more about these tuning parameters.</p><h2 id=controller-improvements>Controller improvements</h2><p>Some community users <a href=https://github.com/kubewarden/kubewarden-controller/issues/645>reported</a> that the Kubewarden Controller was
generating too many requests against the Kubernetes API server.</p><p>During the 1.13 development cycle, we changed the way the Kubewarden Controller interacts with the Kubernetes API server. We removed
some technical debt and then we optimized how we make use of the Kubernetes client-go library.</p><p>That lead to a 50% reduction in the number of requests done by the Kubewarden Controller.</p><p>We plan more work in this area during future releases.</p><p>Finally, by addressing the technical debt we have also been able to improve the code coverage rate of the Kubewarden Controller&rsquo;s codebase.</p><h2 id=a-new-exciting-policy-coming-soon>A new exciting policy coming soon</h2><p>During the 1.13 development cycle, we have been working on a new policy that will be released soon. This policy is worth a dedicated blog post,
so stay tuned!</p><h2 id=lets-stay-in-touch>Let&rsquo;s stay in touch!</h2><p>As always, we are curious about what features you would like next and how you are
enjoying Kubewarden. Reach out on <a href="https://kubernetes.slack.com/?redir=%2Fmessages%2Fkubewarden">Slack</a>
or join our <a href="https://teamup.com/ks2bj74dvw132mhjtj?view=a&showProfileAndInfo=0&showSidepanel=1&disableSidepanel=1&showMenu=1&showAgendaHeader=1&showAgendaDetails=0&showYearViewHeader=1">monthly community meeting</a>
to talk all things Kubewarden.</p></div><div><ul id=tags></ul></div><div></div></article></section></main><footer><p>&copy; 2025 Kubewarden Project Authors. All rights reserved.
The Linux Foundation has registered trademarks and uses trademarks.
For a list of trademarks of The Linux Foundation, please see our Trademark Usage page:
<a href=https://www.linuxfoundation.org/trademark-usage>https://www.linuxfoundation.org/trademark-usage</a></p></footer><script async src="https://www.googletagmanager.com/gtag/js?id=G-PSW07XK7TM"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','G-PSW07XK7TM',{anonymize_ip:!0})}</script></body></html>